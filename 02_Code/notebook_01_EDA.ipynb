{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "AC8pKq_r-3UP",
        "outputId": "86e4491e-63aa-473e-e5bd-41c70ad7a01a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ASD DETECTION PROJECT: BASELINE XGBOOST IMPLEMENTATION\n",
            "Cell 1: Library Setup & Dataset Path Configuration\n",
            "================================================================================\n",
            "\n",
            "[DRIVE SETUP] Mounting Google Drive...\n",
            "--------------------------------------------------\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully\n",
            "\n",
            "[PROJECT STRUCTURE] Validating and creating project paths...\n",
            "--------------------------------------------------\n",
            "Creating missing directories...\n",
            "Found root: /content/drive/MyDrive/ASD_GWO_XGBoost_Project\n",
            "Found dataset: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/01_Dataset\n",
            "Found splits: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/01_Dataset/splits/no_ethnicity\n",
            "Found preprocessed: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/01_Dataset/splits/no_ethnicity/preprocessed\n",
            "Created preprocessors: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/01_Dataset/preprocessors\n",
            "Found results: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/03_Results\n",
            "Found baseline_results: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/03_Results/baseline_experiments\n",
            "\n",
            "Verifying cleaned split files...\n",
            "Found train_set_clean.csv: 0.11 MB\n",
            "Found val_set_clean.csv: 0.03 MB\n",
            "Found test_set_clean.csv: 0.03 MB\n",
            "\n",
            "Using cleaned split files (ethnicity removed)\n",
            "\n",
            "[IMPORTS] Importing essential libraries...\n",
            "--------------------------------------------------\n",
            "Essential libraries imported successfully\n",
            "\n",
            "[CONFIGURATION] Setting up environment...\n",
            "--------------------------------------------------\n",
            "Environment configured successfully\n",
            "\n",
            "[UTILITIES] Defining utility functions...\n",
            "--------------------------------------------------\n",
            "Utility functions defined successfully\n",
            "\n",
            "[VALIDATION] System validation and data assessment...\n",
            "--------------------------------------------------\n",
            "Python version: 3.12.11\n",
            "Package versions:\n",
            "  pandas: 2.2.2\n",
            "  numpy: 2.0.2\n",
            "  scikit-learn: 1.6.1\n",
            "  xgboost: 3.0.5\n",
            "GPU utilities not available\n",
            "\n",
            "Performing data assessment...\n",
            "No ethnicity-related features detected\n",
            "Data assessment completed\n",
            "\n",
            "[EXPORT] Saving setup information...\n",
            "--------------------------------------------------\n",
            "Setup information saved: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/03_Results/baseline_experiments/setup_info.json\n",
            "\n",
            "[SUMMARY] Cell 1 Setup Summary\n",
            "================================================================================\n",
            "CONFIGURATION STATUS:\n",
            "  Project root: /content/drive/MyDrive/ASD_GWO_XGBoost_Project\n",
            "  File status: cleaned\n",
            "  train set: 1270 samples x 26 features\n",
            "  val set: 318 samples x 26 features\n",
            "  test set: 397 samples x 26 features\n",
            "  Target column: ASD_traits\n",
            "  Random state: 42\n",
            "\n",
            "PACKAGE STATUS:\n",
            "  pandas: 2.2.2\n",
            "  numpy: 2.0.2\n",
            "  scikit-learn: 1.6.1\n",
            "  xgboost: 3.0.5\n",
            "\n",
            "BIAS ASSESSMENT:\n",
            "  Ethnicity features detected: 0\n",
            "  Bias risk: low\n",
            "  Recommendation: Proceed with current dataset\n",
            "\n",
            "FILE AVAILABILITY:\n",
            "  Using ethnicity-removed split files\n",
            "  Ready for bias-free analysis\n",
            "\n",
            "NEXT STEPS:\n",
            "  Cell 2: Load cleaned splits, perform EDA, and generate preprocessed CSV files\n",
            "  Cell 3: Baseline 1 - XGBoost with all features (using preprocessed data)\n",
            "  Cell 4: Baseline 2 - XGBoost with feature selection (using preprocessed data)\n",
            "\n",
            "================================================================================\n",
            "CELL 1 COMPLETED SUCCESSFULLY\n",
            "Variables and paths ready for Cell 2\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# @title Cell 1: Library Setup & Dataset Path Configuration\n",
        "\n",
        "\"\"\"\n",
        "ASD Detection Project: Baseline XGBoost Implementation\n",
        "Cell 1: Environment setup with cleaned train/val/test splits (ethnicity removed)\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ASD DETECTION PROJECT: BASELINE XGBOOST IMPLEMENTATION\")\n",
        "print(\"Cell 1: Library Setup & Dataset Path Configuration\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ==========================================\n",
        "# 1. GOOGLE DRIVE MOUNTING\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[DRIVE SETUP] Mounting Google Drive...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Drive mounting failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# ==========================================\n",
        "# 2. PROJECT PATH CONFIGURATION & SETUP\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[PROJECT STRUCTURE] Validating and creating project paths...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "import os\n",
        "\n",
        "# Base project path\n",
        "PROJECT_ROOT = '/content/drive/MyDrive/ASD_GWO_XGBoost_Project'\n",
        "\n",
        "# Define project structure\n",
        "PROJECT_PATHS = {\n",
        "    'root': PROJECT_ROOT,\n",
        "    'dataset': f\"{PROJECT_ROOT}/01_Dataset\",\n",
        "    'splits': f\"{PROJECT_ROOT}/01_Dataset/splits/no_ethnicity\",\n",
        "    'preprocessed': f\"{PROJECT_ROOT}/01_Dataset/splits/no_ethnicity/preprocessed\",\n",
        "    'preprocessors': f\"{PROJECT_ROOT}/01_Dataset/preprocessors\",\n",
        "    'results': f\"{PROJECT_ROOT}/03_Results\",\n",
        "    'baseline_results': f\"{PROJECT_ROOT}/03_Results/baseline_experiments\"\n",
        "}\n",
        "\n",
        "# Create missing directories\n",
        "print(\"Creating missing directories...\")\n",
        "for name, path in PROJECT_PATHS.items():\n",
        "    if os.path.exists(path):\n",
        "        print(f\"Found {name}: {path}\")\n",
        "    else:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        print(f\"Created {name}: {path}\")\n",
        "\n",
        "# Define cleaned split file paths (ethnicity removed)\n",
        "SPLIT_FILES = {\n",
        "    'train': f\"{PROJECT_PATHS['splits']}/train_set_clean.csv\",\n",
        "    'val': f\"{PROJECT_PATHS['splits']}/val_set_clean.csv\",\n",
        "    'test': f\"{PROJECT_PATHS['splits']}/test_set_clean.csv\"\n",
        "}\n",
        "\n",
        "# Verify cleaned split files exist\n",
        "print(\"\\nVerifying cleaned split files...\")\n",
        "for split_name, file_path in SPLIT_FILES.items():\n",
        "    if os.path.exists(file_path):\n",
        "        file_size = os.path.getsize(file_path) / (1024**2)  # MB\n",
        "        print(f\"Found {split_name}_set_clean.csv: {file_size:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"ERROR: Missing {split_name}_set_clean.csv\")\n",
        "        raise FileNotFoundError(f\"Required file not found: {file_path}\")\n",
        "\n",
        "FILE_STATUS = \"cleaned\"\n",
        "print(f\"\\nUsing cleaned split files (ethnicity removed)\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. LIBRARY IMPORTS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[IMPORTS] Importing essential libraries...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Core data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine learning libraries\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectKBest, RFE, f_classif\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# XGBoost\n",
        "import xgboost as xgb\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Utility libraries\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "print(\"Essential libraries imported successfully\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. CONFIGURATION SETUP\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[CONFIGURATION] Setting up environment...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Reproducibility configuration\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# Display configuration\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Visualization configuration\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Project configuration\n",
        "CONFIG = {\n",
        "    'random_state': RANDOM_STATE,\n",
        "    'test_size': 0.2,\n",
        "    'val_size': 0.16,\n",
        "    'train_size': 0.64,\n",
        "    'cv_folds': 5,\n",
        "    'scoring_metric': 'roc_auc',\n",
        "    'target_column': 'ASD_traits'\n",
        "}\n",
        "\n",
        "# XGBoost configuration for baseline\n",
        "XGBOOST_CONFIG = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'auc',\n",
        "    'random_state': RANDOM_STATE,\n",
        "    'n_jobs': -1,\n",
        "    'verbosity': 0\n",
        "}\n",
        "\n",
        "print(\"Environment configured successfully\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. UTILITY FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[UTILITIES] Defining utility functions...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def create_results_directory(experiment_name: str) -> str:\n",
        "    \"\"\"Create and return results directory for experiment\"\"\"\n",
        "    results_dir = f\"{PROJECT_PATHS['baseline_results']}/{experiment_name}\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    return results_dir\n",
        "\n",
        "def save_results(results: dict, filepath: str):\n",
        "    \"\"\"Save results dictionary to JSON file\"\"\"\n",
        "    def json_serializer(obj):\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif pd.isna(obj):\n",
        "            return None\n",
        "        return str(obj)\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(results, f, indent=2, default=json_serializer)\n",
        "\n",
        "def load_split_data(split_name: str) -> pd.DataFrame:\n",
        "    \"\"\"Load specific split data (train/val/test)\"\"\"\n",
        "    file_path = SPLIT_FILES[split_name]\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "def validate_data_consistency():\n",
        "    \"\"\"Validate that all splits have consistent columns and target distribution\"\"\"\n",
        "    splits_info = {}\n",
        "\n",
        "    for split_name in ['train', 'val', 'test']:\n",
        "        try:\n",
        "            df = load_split_data(split_name)\n",
        "            splits_info[split_name] = {\n",
        "                'shape': df.shape,\n",
        "                'columns': list(df.columns),\n",
        "                'target_dist': df[CONFIG['target_column']].value_counts().to_dict() if CONFIG['target_column'] in df.columns else None,\n",
        "                'file_path': SPLIT_FILES[split_name]\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {split_name} set: {e}\")\n",
        "            splits_info[split_name] = {'error': str(e)}\n",
        "\n",
        "    return splits_info\n",
        "\n",
        "def assess_ethnicity_bias():\n",
        "    \"\"\"Assess whether ethnicity features are present (bias indicator)\"\"\"\n",
        "    bias_assessment = {\n",
        "        'ethnicity_features_found': [],\n",
        "        'bias_risk': 'unknown',\n",
        "        'recommendation': 'unknown'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Check train set for ethnicity-related columns\n",
        "        train_df = load_split_data('train')\n",
        "        ethnicity_cols = [col for col in train_df.columns if 'ethnicity' in col.lower()]\n",
        "\n",
        "        bias_assessment['ethnicity_features_found'] = ethnicity_cols\n",
        "\n",
        "        if len(ethnicity_cols) > 0:\n",
        "            bias_assessment['bias_risk'] = 'high'\n",
        "            bias_assessment['recommendation'] = 'Run ethnicity removal script before proceeding'\n",
        "            print(f\"WARNING: {len(ethnicity_cols)} ethnicity-related features detected\")\n",
        "            print(f\"Features: {ethnicity_cols}\")\n",
        "        else:\n",
        "            bias_assessment['bias_risk'] = 'low'\n",
        "            bias_assessment['recommendation'] = 'Proceed with current dataset'\n",
        "            print(\"No ethnicity-related features detected\")\n",
        "\n",
        "    except Exception as e:\n",
        "        bias_assessment['error'] = str(e)\n",
        "        print(f\"Bias assessment failed: {e}\")\n",
        "\n",
        "    return bias_assessment\n",
        "\n",
        "def create_setup_summary():\n",
        "    \"\"\"Create comprehensive setup summary\"\"\"\n",
        "    return {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'project_paths': PROJECT_PATHS,\n",
        "        'split_files': SPLIT_FILES,\n",
        "        'file_status': FILE_STATUS,\n",
        "        'configuration': CONFIG,\n",
        "        'xgboost_config': XGBOOST_CONFIG,\n",
        "        'package_versions': packages,\n",
        "        'splits_info': SPLITS_INFO if 'SPLITS_INFO' in globals() else {},\n",
        "        'bias_assessment': BIAS_ASSESSMENT if 'BIAS_ASSESSMENT' in globals() else {},\n",
        "        'cell_status': 'completed'\n",
        "    }\n",
        "\n",
        "print(\"Utility functions defined successfully\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. SYSTEM VALIDATION & DATA ASSESSMENT\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[VALIDATION] System validation and data assessment...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Check Python version\n",
        "import sys\n",
        "print(f\"Python version: {sys.version.split()[0]}\")\n",
        "\n",
        "# Check key package versions\n",
        "packages = {\n",
        "    'pandas': pd.__version__,\n",
        "    'numpy': np.__version__,\n",
        "    'scikit-learn': __import__('sklearn').__version__,\n",
        "    'xgboost': xgb.__version__\n",
        "}\n",
        "\n",
        "print(\"Package versions:\")\n",
        "for pkg, version in packages.items():\n",
        "    print(f\"  {pkg}: {version}\")\n",
        "\n",
        "# Validate GPU availability\n",
        "try:\n",
        "    import GPUtil\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        print(f\"GPU available: {gpus[0].name}\")\n",
        "    else:\n",
        "        print(\"No GPU detected\")\n",
        "except:\n",
        "    print(\"GPU utilities not available\")\n",
        "\n",
        "# Data consistency and bias assessment\n",
        "print(\"\\nPerforming data assessment...\")\n",
        "try:\n",
        "    splits_info = validate_data_consistency()\n",
        "\n",
        "    # Check for ethnicity bias indicators\n",
        "    bias_assessment = assess_ethnicity_bias()\n",
        "\n",
        "    print(\"Data assessment completed\")\n",
        "\n",
        "    # Store for next cell\n",
        "    globals()['SPLITS_INFO'] = splits_info\n",
        "    globals()['BIAS_ASSESSMENT'] = bias_assessment\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Data assessment failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# ==========================================\n",
        "# 7. EXPORT SETUP INFORMATION\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[EXPORT] Saving setup information...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Create baseline results directory\n",
        "os.makedirs(PROJECT_PATHS['baseline_results'], exist_ok=True)\n",
        "\n",
        "# Setup summary\n",
        "setup_info = create_setup_summary()\n",
        "\n",
        "# Save setup info\n",
        "setup_path = f\"{PROJECT_PATHS['baseline_results']}/setup_info.json\"\n",
        "save_results(setup_info, setup_path)\n",
        "\n",
        "print(f\"Setup information saved: {setup_path}\")\n",
        "\n",
        "# ==========================================\n",
        "# 8. COMPLETION SUMMARY\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[SUMMARY] Cell 1 Setup Summary\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"CONFIGURATION STATUS:\")\n",
        "print(f\"  Project root: {PROJECT_ROOT}\")\n",
        "print(f\"  File status: {FILE_STATUS}\")\n",
        "\n",
        "if 'SPLITS_INFO' in globals():\n",
        "    for split_name, info in SPLITS_INFO.items():\n",
        "        if 'shape' in info:\n",
        "            print(f\"  {split_name} set: {info['shape'][0]} samples x {info['shape'][1]} features\")\n",
        "        else:\n",
        "            print(f\"  {split_name} set: Error loading\")\n",
        "\n",
        "print(f\"  Target column: {CONFIG['target_column']}\")\n",
        "print(f\"  Random state: {CONFIG['random_state']}\")\n",
        "\n",
        "print(\"\\nPACKAGE STATUS:\")\n",
        "for pkg, version in packages.items():\n",
        "    print(f\"  {pkg}: {version}\")\n",
        "\n",
        "if 'BIAS_ASSESSMENT' in globals():\n",
        "    print(f\"\\nBIAS ASSESSMENT:\")\n",
        "    print(f\"  Ethnicity features detected: {len(BIAS_ASSESSMENT.get('ethnicity_features_found', []))}\")\n",
        "    print(f\"  Bias risk: {BIAS_ASSESSMENT.get('bias_risk', 'unknown')}\")\n",
        "    print(f\"  Recommendation: {BIAS_ASSESSMENT.get('recommendation', 'unknown')}\")\n",
        "\n",
        "print(\"\\nFILE AVAILABILITY:\")\n",
        "print(\"  Using ethnicity-removed split files\")\n",
        "print(\"  Ready for bias-free analysis\")\n",
        "\n",
        "print(\"\\nNEXT STEPS:\")\n",
        "print(\"  Cell 2: Load cleaned splits, perform EDA, and generate preprocessed CSV files\")\n",
        "print(\"  Cell 3: Baseline 1 - XGBoost with all features (using preprocessed data)\")\n",
        "print(\"  Cell 4: Baseline 2 - XGBoost with feature selection (using preprocessed data)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CELL 1 COMPLETED SUCCESSFULLY\")\n",
        "print(\"Variables and paths ready for Cell 2\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 2: EDA & Preprocessing Pipeline\n",
        "\n",
        "\"\"\"\n",
        "ASD Detection Project: Baseline XGBoost Implementation\n",
        "Cell 2: Load cleaned splits, perform publication-ready EDA, and generate preprocessed CSV files\n",
        "Enhanced for IEEE conference standard with comprehensive clinical analysis\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ASD DETECTION PROJECT: BASELINE XGBOOST IMPLEMENTATION\")\n",
        "print(\"Cell 2: Enhanced EDA & Preprocessing Pipeline (Publication-Ready)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ==========================================\n",
        "# 1. VERIFY CELL 1 EXECUTION\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[VERIFICATION] Checking Cell 1 completion...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "try:\n",
        "    # Verify essential variables from Cell 1\n",
        "    PROJECT_PATHS\n",
        "    SPLIT_FILES\n",
        "    CONFIG\n",
        "    XGBOOST_CONFIG\n",
        "    SPLITS_INFO\n",
        "    print(\"Cell 1 variables loaded successfully\")\n",
        "except NameError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Please run Cell 1 first\")\n",
        "    raise Exception(\"Cell 1 must be executed before Cell 2\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. LOAD CLEANED SPLIT DATASETS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[DATA LOADING] Loading cleaned split datasets...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def load_all_splits():\n",
        "    \"\"\"Load all three cleaned split datasets\"\"\"\n",
        "    splits = {}\n",
        "\n",
        "    for split_name in ['train', 'val', 'test']:\n",
        "        try:\n",
        "            df = load_split_data(split_name)\n",
        "            splits[split_name] = df\n",
        "            print(f\"Loaded {split_name} set: {df.shape[0]:,} samples x {df.shape[1]} features\")\n",
        "\n",
        "            # Verify target column exists\n",
        "            if CONFIG['target_column'] not in df.columns:\n",
        "                raise ValueError(f\"Target column '{CONFIG['target_column']}' not found in {split_name} set\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {split_name} set: {e}\")\n",
        "            raise\n",
        "\n",
        "    return splits\n",
        "\n",
        "# Load all splits\n",
        "SPLITS_DATA = load_all_splits()\n",
        "\n",
        "# Separate features and targets for each split\n",
        "def prepare_feature_target_splits(splits_data, target_col):\n",
        "    \"\"\"Separate features and targets for all splits\"\"\"\n",
        "    prepared_splits = {}\n",
        "\n",
        "    for split_name, df in splits_data.items():\n",
        "        X = df.drop(columns=[target_col])\n",
        "        y = df[target_col]\n",
        "        prepared_splits[split_name] = {'X': X, 'y': y, 'df': df}\n",
        "\n",
        "    return prepared_splits\n",
        "\n",
        "PREPARED_SPLITS = prepare_feature_target_splits(SPLITS_DATA, CONFIG['target_column'])\n",
        "\n",
        "print(f\"\\nFeature-target separation completed\")\n",
        "print(f\"Feature columns: {PREPARED_SPLITS['train']['X'].shape[1]}\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. PUBLICATION-READY EDA ANALYSIS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[PUBLICATION EDA] Comprehensive clinical analysis (train set only)...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def comprehensive_clinical_eda():\n",
        "    \"\"\"Publication-ready EDA with clinical focus and statistical rigor\"\"\"\n",
        "\n",
        "    train_df = PREPARED_SPLITS['train']['df']\n",
        "    X_train = PREPARED_SPLITS['train']['X']\n",
        "    y_train = PREPARED_SPLITS['train']['y']\n",
        "\n",
        "    clinical_eda = {\n",
        "        'dataset_overview': {},\n",
        "        'clinical_target_analysis': {},\n",
        "        'behavioral_features_analysis': {},\n",
        "        'clinical_assessment_analysis': {},\n",
        "        'demographic_analysis': {},\n",
        "        'statistical_tests': {}\n",
        "    }\n",
        "\n",
        "    # Section 1: Dataset Overview & Quality Assessment\n",
        "    print(\"Section 1: Dataset Overview & Quality Assessment\")\n",
        "\n",
        "    # Basic demographics\n",
        "    dataset_overview = {\n",
        "        'total_samples': len(train_df),\n",
        "        'total_features': len(X_train.columns),\n",
        "        'memory_usage_mb': train_df.memory_usage(deep=True).sum() / 1024**2,\n",
        "        'data_quality': {\n",
        "            'missing_values': train_df.isnull().sum().sum(),\n",
        "            'duplicate_rows': train_df.duplicated().sum()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    clinical_eda['dataset_overview'] = dataset_overview\n",
        "\n",
        "    # Section 2: Clinical Target Analysis (Priority Section)\n",
        "    print(\"Section 2: Clinical Target Analysis\")\n",
        "\n",
        "    target_analysis = {}\n",
        "\n",
        "    # ASD prevalence across splits\n",
        "    prevalence_by_split = {}\n",
        "    for split_name in ['train', 'val', 'test']:\n",
        "        split_y = PREPARED_SPLITS[split_name]['y']\n",
        "        target_dist = split_y.value_counts()\n",
        "        target_props = split_y.value_counts(normalize=True)\n",
        "\n",
        "        prevalence_by_split[split_name] = {\n",
        "            'total_samples': len(split_y),\n",
        "            'asd_count': target_dist.get(1, 0),\n",
        "            'non_asd_count': target_dist.get(0, 0),\n",
        "            'asd_prevalence': target_props.get(1, 0),\n",
        "            'balance_ratio': min(target_dist) / max(target_dist) if len(target_dist) > 1 else 1.0\n",
        "        }\n",
        "\n",
        "    target_analysis['prevalence_by_split'] = prevalence_by_split\n",
        "\n",
        "    # Age-stratified analysis if Age column exists\n",
        "    age_cols = [col for col in X_train.columns if 'age' in col.lower()]\n",
        "    if age_cols:\n",
        "        age_col = age_cols[0]\n",
        "        age_stratified = {}\n",
        "\n",
        "        # Define age groups for pediatric population\n",
        "        age_groups = pd.cut(X_train[age_col], bins=[0, 3, 6, 10, 15, 18],\n",
        "                           labels=['0-3', '4-6', '7-10', '11-15', '16-18'])\n",
        "\n",
        "        for age_group in age_groups.cat.categories:\n",
        "            mask = age_groups == age_group\n",
        "            if mask.sum() > 0:\n",
        "                group_y = y_train[mask]\n",
        "                if len(group_y) > 0:\n",
        "                    asd_rate = (group_y == 1).mean()\n",
        "                    age_stratified[age_group] = {\n",
        "                        'total': len(group_y),\n",
        "                        'asd_count': (group_y == 1).sum(),\n",
        "                        'asd_rate': asd_rate\n",
        "                    }\n",
        "\n",
        "        target_analysis['age_stratified'] = age_stratified\n",
        "\n",
        "    clinical_eda['clinical_target_analysis'] = target_analysis\n",
        "\n",
        "    # Section 3: Behavioral Features Analysis (A1-A10) - Priority Section\n",
        "    print(\"Section 3: Behavioral Features Analysis (A1-A10)\")\n",
        "\n",
        "    behavioral_analysis = {}\n",
        "\n",
        "    # Identify A1-A10 features\n",
        "    behavioral_features = [col for col in X_train.columns if\n",
        "                          col.startswith('A') and len(col) <= 3 and\n",
        "                          any(char.isdigit() for char in col)]\n",
        "    behavioral_features.sort()\n",
        "\n",
        "    if behavioral_features:\n",
        "        behavioral_stats = {}\n",
        "\n",
        "        for feature in behavioral_features:\n",
        "            feature_data = X_train[feature]\n",
        "\n",
        "            # Basic statistics by ASD status\n",
        "            asd_group = feature_data[y_train == 1]\n",
        "            non_asd_group = feature_data[y_train == 0]\n",
        "\n",
        "            # Effect size calculation (Cohen's d)\n",
        "            def cohens_d(group1, group2):\n",
        "                n1, n2 = len(group1), len(group2)\n",
        "                s1, s2 = group1.std(), group2.std()\n",
        "                s_pooled = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n",
        "                return (group1.mean() - group2.mean()) / s_pooled if s_pooled != 0 else 0\n",
        "\n",
        "            # Statistical test\n",
        "            from scipy.stats import chi2_contingency, ttest_ind, mannwhitneyu\n",
        "\n",
        "            stat_test_result = {}\n",
        "            try:\n",
        "                if feature_data.nunique() <= 2:  # Binary feature\n",
        "                    contingency = pd.crosstab(y_train, feature_data)\n",
        "                    if contingency.size > 1 and contingency.min().min() > 0:  # Avoid cells with 0\n",
        "                        chi2, p_val, _, _ = chi2_contingency(contingency)\n",
        "                        stat_test_result = {\n",
        "                            'test': 'chi_square',\n",
        "                            'statistic': float(chi2),\n",
        "                            'p_value': float(p_val),\n",
        "                            'significant': p_val < 0.05\n",
        "                        }\n",
        "                    else:\n",
        "                        stat_test_result = {\n",
        "                            'test': 'chi_square',\n",
        "                            'error': 'Insufficient data for test',\n",
        "                            'significant': False\n",
        "                        }\n",
        "                else:  # Continuous feature\n",
        "                    if len(asd_group) > 0 and len(non_asd_group) > 0:\n",
        "                        # Check normality assumption (simplified)\n",
        "                        if len(asd_group) >= 30 and len(non_asd_group) >= 30:\n",
        "                            stat, p_val = ttest_ind(asd_group, non_asd_group)\n",
        "                            test_name = 't_test'\n",
        "                        else:\n",
        "                            stat, p_val = mannwhitneyu(asd_group, non_asd_group)\n",
        "                            test_name = 'mann_whitney'\n",
        "\n",
        "                        stat_test_result = {\n",
        "                            'test': test_name,\n",
        "                            'statistic': float(stat),\n",
        "                            'p_value': float(p_val),\n",
        "                            'significant': p_val < 0.05\n",
        "                        }\n",
        "                    else:\n",
        "                        stat_test_result = {\n",
        "                            'test': 'insufficient_data',\n",
        "                            'error': 'Empty groups',\n",
        "                            'significant': False\n",
        "                        }\n",
        "            except Exception as e:\n",
        "                stat_test_result = {\n",
        "                    'test': 'failed',\n",
        "                    'error': str(e),\n",
        "                    'significant': False\n",
        "                }\n",
        "\n",
        "            behavioral_stats[feature] = {\n",
        "                'asd_mean': asd_group.mean() if len(asd_group) > 0 else 0,\n",
        "                'non_asd_mean': non_asd_group.mean() if len(non_asd_group) > 0 else 0,\n",
        "                'asd_std': asd_group.std() if len(asd_group) > 0 else 0,\n",
        "                'non_asd_std': non_asd_group.std() if len(non_asd_group) > 0 else 0,\n",
        "                'cohens_d': cohens_d(asd_group, non_asd_group) if len(asd_group) > 0 and len(non_asd_group) > 0 else 0,\n",
        "                'statistical_test': stat_test_result\n",
        "            }\n",
        "\n",
        "        # Behavioral pattern analysis\n",
        "        behavioral_correlation = X_train[behavioral_features].corr()\n",
        "        behavioral_total_score = X_train[behavioral_features].sum(axis=1)\n",
        "\n",
        "        behavioral_analysis = {\n",
        "            'features_analyzed': behavioral_features,\n",
        "            'individual_statistics': behavioral_stats,\n",
        "            'internal_correlation_mean': behavioral_correlation.values[np.triu_indices_from(behavioral_correlation.values, k=1)].mean(),\n",
        "            'total_score_stats': {\n",
        "                'asd_mean': behavioral_total_score[y_train == 1].mean(),\n",
        "                'non_asd_mean': behavioral_total_score[y_train == 0].mean(),\n",
        "                'asd_std': behavioral_total_score[y_train == 1].std(),\n",
        "                'non_asd_std': behavioral_total_score[y_train == 0].std()\n",
        "            }\n",
        "        }\n",
        "\n",
        "    clinical_eda['behavioral_features_analysis'] = behavioral_analysis\n",
        "\n",
        "    # Section 4: Clinical Assessment Scales - Priority Section\n",
        "    print(\"Section 4: Clinical Assessment Scales\")\n",
        "\n",
        "    assessment_analysis = {}\n",
        "\n",
        "    # Identify clinical scales\n",
        "    scale_features = {\n",
        "        'CARS': [col for col in X_train.columns if 'CARS' in col or 'Rating' in col],\n",
        "        'SRS': [col for col in X_train.columns if 'SRS' in col or 'Social' in col and 'Scale' in col],\n",
        "        'AQ10': [col for col in X_train.columns if 'AQ' in col or 'Qchat' in col or 'Score' in col]\n",
        "    }\n",
        "\n",
        "    for scale_name, scale_cols in scale_features.items():\n",
        "        if scale_cols:\n",
        "            scale_col = scale_cols[0]  # Take first matching column\n",
        "            scale_data = X_train[scale_col]\n",
        "\n",
        "            # Distribution analysis by ASD status\n",
        "            asd_scores = scale_data[y_train == 1]\n",
        "            non_asd_scores = scale_data[y_train == 0]\n",
        "\n",
        "            # Statistical comparison\n",
        "            if len(asd_scores) > 0 and len(non_asd_scores) > 0:\n",
        "                stat_result = {}\n",
        "                try:\n",
        "                    if len(asd_scores) >= 30 and len(non_asd_scores) >= 30:\n",
        "                        stat, p_val = ttest_ind(asd_scores, non_asd_scores)\n",
        "                        test_name = 't_test'\n",
        "                    else:\n",
        "                        stat, p_val = mannwhitneyu(asd_scores, non_asd_scores)\n",
        "                        test_name = 'mann_whitney'\n",
        "\n",
        "                    stat_result = {\n",
        "                        'test': test_name,\n",
        "                        'statistic': stat,\n",
        "                        'p_value': p_val,\n",
        "                        'significant': p_val < 0.05\n",
        "                    }\n",
        "                except:\n",
        "                    stat_result = {'test': 'failed', 'error': 'Insufficient data'}\n",
        "\n",
        "                assessment_analysis[scale_name] = {\n",
        "                    'column': scale_col,\n",
        "                    'asd_mean': asd_scores.mean(),\n",
        "                    'non_asd_mean': non_asd_scores.mean(),\n",
        "                    'asd_std': asd_scores.std(),\n",
        "                    'non_asd_std': non_asd_scores.std(),\n",
        "                    'statistical_test': stat_result,\n",
        "                    'score_range': {\n",
        "                        'min': scale_data.min(),\n",
        "                        'max': scale_data.max(),\n",
        "                        'unique_values': scale_data.nunique()\n",
        "                    }\n",
        "                }\n",
        "\n",
        "    clinical_eda['clinical_assessment_analysis'] = assessment_analysis\n",
        "\n",
        "    # Section 5: Demographic Analysis\n",
        "    print(\"Section 5: Demographic Analysis\")\n",
        "\n",
        "    demographic_analysis = {}\n",
        "\n",
        "    # Gender analysis\n",
        "    gender_cols = [col for col in X_train.columns if any(term in col.lower() for term in ['sex', 'gender'])]\n",
        "    if gender_cols:\n",
        "        gender_col = gender_cols[0]\n",
        "        gender_crosstab = pd.crosstab(X_train[gender_col], y_train)\n",
        "\n",
        "        # Chi-square test for gender association\n",
        "        try:\n",
        "            chi2, p_val, _, _ = chi2_contingency(gender_crosstab)\n",
        "            gender_test = {\n",
        "                'test': 'chi_square',\n",
        "                'statistic': chi2,\n",
        "                'p_value': p_val,\n",
        "                'significant': p_val < 0.05\n",
        "            }\n",
        "        except:\n",
        "            gender_test = {'test': 'failed', 'error': 'Insufficient data'}\n",
        "\n",
        "        demographic_analysis['gender'] = {\n",
        "            'column': gender_col,\n",
        "            'crosstab': gender_crosstab.to_dict(),\n",
        "            'statistical_test': gender_test\n",
        "        }\n",
        "\n",
        "    clinical_eda['demographic_analysis'] = demographic_analysis\n",
        "\n",
        "    return clinical_eda\n",
        "\n",
        "# Perform comprehensive clinical EDA\n",
        "CLINICAL_EDA = comprehensive_clinical_eda()\n",
        "\n",
        "# ==========================================\n",
        "# 4. ENHANCED REDUNDANCY ANALYSIS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[REDUNDANCY ANALYSIS] Comprehensive feature redundancy assessment...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def comprehensive_redundancy_analysis(X_train, correlation_threshold=0.95, variance_threshold=0.01):\n",
        "    \"\"\"Comprehensive redundancy analysis with clinical considerations\"\"\"\n",
        "\n",
        "    redundancy_report = {\n",
        "        'correlation_analysis': {},\n",
        "        'variance_analysis': {},\n",
        "        'duplicate_analysis': {},\n",
        "        'clinical_analysis': {}\n",
        "    }\n",
        "\n",
        "    # 1. Enhanced correlation analysis with data type safety\n",
        "    print(\"Step 1: Correlation analysis\")\n",
        "\n",
        "    # Get truly numeric columns\n",
        "    numeric_cols = []\n",
        "    for col in X_train.columns:\n",
        "        if pd.api.types.is_numeric_dtype(X_train[col]):\n",
        "            try:\n",
        "                pd.to_numeric(X_train[col], errors='raise')\n",
        "                numeric_cols.append(col)\n",
        "            except (ValueError, TypeError):\n",
        "                print(f\"  Skipping {col}: contains non-numeric values\")\n",
        "        else:\n",
        "            # Check if it's a binary categorical that can be safely converted\n",
        "            unique_vals = X_train[col].dropna().unique()\n",
        "            if len(unique_vals) <= 2 and all(val in ['Yes', 'No', 'yes', 'no', '1', '0', 1, 0, True, False] for val in unique_vals):\n",
        "                print(f\"  Converting binary categorical {col} for analysis\")\n",
        "                try:\n",
        "                    temp_col = X_train[col].map({'Yes': 1, 'No': 0, 'yes': 1, 'no': 0, '1': 1, '0': 0, 1: 1, 0: 0, True: 1, False: 0})\n",
        "                    if not temp_col.isnull().all():\n",
        "                        numeric_cols.append(col)\n",
        "                except:\n",
        "                    print(f\"  Skipping {col}: conversion failed\")\n",
        "\n",
        "    print(f\"  Analyzing {len(numeric_cols)} numeric/binary features for correlation\")\n",
        "\n",
        "    if len(numeric_cols) > 1:\n",
        "        corr_data = X_train[numeric_cols].copy()\n",
        "\n",
        "        # Convert binary categorical columns for correlation analysis\n",
        "        for col in corr_data.columns:\n",
        "            if not pd.api.types.is_numeric_dtype(corr_data[col]):\n",
        "                corr_data[col] = corr_data[col].map({'Yes': 1, 'No': 0, 'yes': 1, 'no': 0})\n",
        "\n",
        "        try:\n",
        "            corr_matrix = corr_data.corr().abs()\n",
        "\n",
        "            # Find high correlation pairs\n",
        "            high_corr_pairs = []\n",
        "            for i in range(len(corr_matrix.columns)):\n",
        "                for j in range(i+1, len(corr_matrix.columns)):\n",
        "                    corr_val = corr_matrix.iloc[i, j]\n",
        "                    if not np.isnan(corr_val) and corr_val > correlation_threshold:\n",
        "                        high_corr_pairs.append({\n",
        "                            'feature1': corr_matrix.columns[i],\n",
        "                            'feature2': corr_matrix.columns[j],\n",
        "                            'correlation': float(corr_val)\n",
        "                        })\n",
        "\n",
        "            print(f\"  Found {len(high_corr_pairs)} high correlation pairs (>{correlation_threshold})\")\n",
        "            redundancy_report['correlation_analysis'] = {\n",
        "                'threshold': correlation_threshold,\n",
        "                'high_corr_pairs': high_corr_pairs,\n",
        "                'correlation_matrix': corr_matrix,\n",
        "                'numeric_columns': numeric_cols\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"  Correlation analysis failed: {e}\")\n",
        "            redundancy_report['correlation_analysis'] = {\n",
        "                'error': str(e),\n",
        "                'threshold': correlation_threshold,\n",
        "                'high_corr_pairs': [],\n",
        "                'numeric_columns': []\n",
        "            }\n",
        "\n",
        "    # 2. Variance analysis\n",
        "    print(\"Step 2: Low variance analysis\")\n",
        "    low_variance_features = []\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        try:\n",
        "            col_data = X_train[col]\n",
        "            if not pd.api.types.is_numeric_dtype(col_data):\n",
        "                col_data = col_data.map({'Yes': 1, 'No': 0, 'yes': 1, 'no': 0})\n",
        "\n",
        "            variance = col_data.var()\n",
        "            if not np.isnan(variance) and variance < variance_threshold:\n",
        "                low_variance_features.append({\n",
        "                    'feature': col,\n",
        "                    'variance': float(variance)\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"  Skipping variance analysis for {col}: {e}\")\n",
        "\n",
        "    print(f\"  Found {len(low_variance_features)} low variance features (<{variance_threshold})\")\n",
        "    redundancy_report['variance_analysis'] = {\n",
        "        'threshold': variance_threshold,\n",
        "        'low_variance_features': low_variance_features\n",
        "    }\n",
        "\n",
        "    # 3. Exact duplicate detection\n",
        "    print(\"Step 3: Duplicate feature detection\")\n",
        "    duplicate_features = []\n",
        "\n",
        "    try:\n",
        "        for i in range(len(X_train.columns)):\n",
        "            for j in range(i+1, len(X_train.columns)):\n",
        "                col1, col2 = X_train.columns[i], X_train.columns[j]\n",
        "                if X_train[col1].equals(X_train[col2]):\n",
        "                    duplicate_features.append({\n",
        "                        'feature1': col1,\n",
        "                        'feature2': col2\n",
        "                    })\n",
        "\n",
        "        print(f\"  Found {len(duplicate_features)} exact duplicate feature pairs\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Duplicate detection failed: {e}\")\n",
        "\n",
        "    redundancy_report['duplicate_analysis'] = {\n",
        "        'exact_duplicates': duplicate_features\n",
        "    }\n",
        "\n",
        "    # 4. Enhanced clinical grouping analysis\n",
        "    print(\"Step 4: Clinical feature grouping\")\n",
        "\n",
        "    clinical_groups = {\n",
        "        'behavioral_core': [col for col in X_train.columns if col.startswith('A') and len(col) >= 2 and col[1:2].isdigit()],\n",
        "        'assessment_scales': [col for col in X_train.columns if any(scale in col for scale in ['Scale', 'Score', 'Rating', 'Qchat'])],\n",
        "        'demographic': [col for col in X_train.columns if any(demo in col.lower() for demo in ['age', 'sex', 'gender'])],\n",
        "        'medical_history': [col for col in X_train.columns if any(med in col.lower() for med in ['disorder', 'delay', 'depression', 'anxiety', 'jaundice'])]\n",
        "    }\n",
        "\n",
        "    # Analyze within-group correlations for numeric groups only\n",
        "    group_correlations = {}\n",
        "    for group_name, group_features in clinical_groups.items():\n",
        "        if len(group_features) > 1:\n",
        "            group_numeric = [col for col in group_features if col in numeric_cols]\n",
        "\n",
        "            if len(group_numeric) > 1:\n",
        "                try:\n",
        "                    group_data = X_train[group_numeric].copy()\n",
        "\n",
        "                    # Convert binary categorical if needed\n",
        "                    for col in group_data.columns:\n",
        "                        if not pd.api.types.is_numeric_dtype(group_data[col]):\n",
        "                            group_data[col] = group_data[col].map({'Yes': 1, 'No': 0, 'yes': 1, 'no': 0})\n",
        "\n",
        "                    group_corr = group_data.corr().abs()\n",
        "                    upper_triangle = group_corr.where(np.triu(np.ones(group_corr.shape), k=1).astype(bool))\n",
        "                    avg_corr = upper_triangle.stack().mean()\n",
        "\n",
        "                    if not np.isnan(avg_corr):\n",
        "                        group_correlations[group_name] = {\n",
        "                            'features': group_features,\n",
        "                            'numeric_features': group_numeric,\n",
        "                            'count': len(group_features),\n",
        "                            'numeric_count': len(group_numeric),\n",
        "                            'avg_internal_correlation': float(avg_corr),\n",
        "                            'correlation_matrix': group_corr\n",
        "                        }\n",
        "                        print(f\"  {group_name}: {len(group_features)} features ({len(group_numeric)} numeric), avg correlation: {avg_corr:.3f}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  {group_name}: correlation analysis error: {e}\")\n",
        "\n",
        "    redundancy_report['clinical_analysis'] = {\n",
        "        'clinical_groups': clinical_groups,\n",
        "        'group_correlations': group_correlations\n",
        "    }\n",
        "\n",
        "    return redundancy_report\n",
        "\n",
        "# Perform enhanced redundancy analysis\n",
        "REDUNDANCY_REPORT = comprehensive_redundancy_analysis(PREPARED_SPLITS['train']['X'])\n",
        "\n",
        "# ==========================================\n",
        "# 5. PREPROCESSING PIPELINE IMPLEMENTATION\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[PREPROCESSING] Implementing fit-transform pipeline...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def create_preprocessing_pipeline(X_train, y_train):\n",
        "    \"\"\"Create preprocessing pipeline fitted on train data only\"\"\"\n",
        "    pipeline_components = {}\n",
        "\n",
        "    # Identify feature types\n",
        "    numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    print(f\"Step 1: Feature type identification\")\n",
        "    print(f\"  Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"  Categorical columns: {len(categorical_cols)}\")\n",
        "\n",
        "    # 1. Missing value imputation\n",
        "    print(\"Step 2: Missing value imputation\")\n",
        "\n",
        "    if numeric_cols:\n",
        "        numeric_imputer = SimpleImputer(strategy='median')\n",
        "        numeric_imputer.fit(X_train[numeric_cols])\n",
        "        pipeline_components['numeric_imputer'] = numeric_imputer\n",
        "        print(f\"  Numeric imputer fitted (median strategy)\")\n",
        "\n",
        "    if categorical_cols:\n",
        "        categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "        categorical_imputer.fit(X_train[categorical_cols])\n",
        "        pipeline_components['categorical_imputer'] = categorical_imputer\n",
        "        print(f\"  Categorical imputer fitted (mode strategy)\")\n",
        "\n",
        "    # 2. Categorical encoding\n",
        "    print(\"Step 3: Categorical encoding\")\n",
        "\n",
        "    if categorical_cols:\n",
        "        label_encoders = {}\n",
        "        for col in categorical_cols:\n",
        "            le = LabelEncoder()\n",
        "            le.fit(X_train[col].astype(str))\n",
        "            label_encoders[col] = le\n",
        "\n",
        "        pipeline_components['label_encoders'] = label_encoders\n",
        "        print(f\"  Label encoders fitted for {len(label_encoders)} categorical features\")\n",
        "\n",
        "    # 3. Feature scaling\n",
        "    print(\"Step 4: Feature scaling\")\n",
        "\n",
        "    # Identify binary features from EDA results\n",
        "    binary_features = []\n",
        "    for col in numeric_cols:\n",
        "        unique_vals = X_train[col].dropna().unique()\n",
        "        if len(unique_vals) == 2 and set(unique_vals).issubset({0, 1, 0.0, 1.0, True, False}):\n",
        "            binary_features.append(col)\n",
        "\n",
        "    scale_cols = [col for col in numeric_cols if col not in binary_features]\n",
        "\n",
        "    if scale_cols:\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_train[scale_cols])\n",
        "        pipeline_components['scaler'] = scaler\n",
        "        pipeline_components['scale_columns'] = scale_cols\n",
        "        print(f\"  Scaler fitted for {len(scale_cols)} continuous features\")\n",
        "\n",
        "    # 4. Target encoding\n",
        "    print(\"Step 5: Target encoding\")\n",
        "\n",
        "    if y_train.dtype == 'object':\n",
        "        target_encoder = LabelEncoder()\n",
        "        target_encoder.fit(y_train)\n",
        "        pipeline_components['target_encoder'] = target_encoder\n",
        "        print(f\"  Target encoder fitted\")\n",
        "\n",
        "    pipeline_components['feature_columns'] = {\n",
        "        'numeric': numeric_cols,\n",
        "        'categorical': categorical_cols,\n",
        "        'scale': scale_cols,\n",
        "        'binary': binary_features\n",
        "    }\n",
        "\n",
        "    return pipeline_components\n",
        "\n",
        "def apply_preprocessing_pipeline(X, y, pipeline_components, split_name):\n",
        "    \"\"\"Apply preprocessing pipeline to any split\"\"\"\n",
        "    X_processed = X.copy()\n",
        "    y_processed = y.copy()\n",
        "\n",
        "    print(f\"Applying preprocessing to {split_name} set...\")\n",
        "\n",
        "    # 1. Apply missing value imputation\n",
        "    if 'numeric_imputer' in pipeline_components:\n",
        "        numeric_cols = pipeline_components['feature_columns']['numeric']\n",
        "        X_processed[numeric_cols] = pipeline_components['numeric_imputer'].transform(X_processed[numeric_cols])\n",
        "\n",
        "    if 'categorical_imputer' in pipeline_components:\n",
        "        categorical_cols = pipeline_components['feature_columns']['categorical']\n",
        "        X_processed[categorical_cols] = pipeline_components['categorical_imputer'].transform(X_processed[categorical_cols])\n",
        "\n",
        "    # 2. Apply categorical encoding\n",
        "    if 'label_encoders' in pipeline_components:\n",
        "        for col, encoder in pipeline_components['label_encoders'].items():\n",
        "            X_processed[col] = encoder.transform(X_processed[col].astype(str))\n",
        "\n",
        "    # 3. Apply feature scaling\n",
        "    if 'scaler' in pipeline_components:\n",
        "        scale_cols = pipeline_components['scale_columns']\n",
        "        X_processed[scale_cols] = pipeline_components['scaler'].transform(X_processed[scale_cols])\n",
        "\n",
        "    # 4. Apply target encoding\n",
        "    if 'target_encoder' in pipeline_components:\n",
        "        y_processed = pipeline_components['target_encoder'].transform(y_processed)\n",
        "\n",
        "    print(f\"  {split_name} preprocessing completed: {X_processed.shape}\")\n",
        "\n",
        "    return X_processed, y_processed\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "PREPROCESSING_PIPELINE = create_preprocessing_pipeline(\n",
        "    PREPARED_SPLITS['train']['X'],\n",
        "    PREPARED_SPLITS['train']['y']\n",
        ")\n",
        "\n",
        "# Apply preprocessing to all splits\n",
        "PROCESSED_SPLITS = {}\n",
        "for split_name in ['train', 'val', 'test']:\n",
        "    X_processed, y_processed = apply_preprocessing_pipeline(\n",
        "        PREPARED_SPLITS[split_name]['X'],\n",
        "        PREPARED_SPLITS[split_name]['y'],\n",
        "        PREPROCESSING_PIPELINE,\n",
        "        split_name\n",
        "    )\n",
        "    PROCESSED_SPLITS[split_name] = {\n",
        "        'X': X_processed,\n",
        "        'y': y_processed\n",
        "    }\n",
        "\n",
        "print(\"Preprocessing pipeline completed for all splits\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. SAVE PREPROCESSED DATA TO CSV\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[SAVE PREPROCESSED] Saving preprocessed datasets to CSV...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Create preprocessed directory\n",
        "preprocessed_dir = PROJECT_PATHS['preprocessed']\n",
        "os.makedirs(preprocessed_dir, exist_ok=True)\n",
        "\n",
        "# Save each preprocessed split\n",
        "for split_name in ['train', 'val', 'test']:\n",
        "    X_processed = PROCESSED_SPLITS[split_name]['X']\n",
        "    y_processed = PROCESSED_SPLITS[split_name]['y']\n",
        "\n",
        "    # Combine features and target\n",
        "    processed_df = X_processed.copy()\n",
        "    processed_df[CONFIG['target_column']] = y_processed\n",
        "\n",
        "    # Save to CSV\n",
        "    output_path = f\"{preprocessed_dir}/{split_name}_set_preprocessed.csv\"\n",
        "    processed_df.to_csv(output_path, index=False)\n",
        "\n",
        "    file_size = os.path.getsize(output_path) / (1024**2)  # MB\n",
        "    print(f\"Saved {split_name}_set_preprocessed.csv: {file_size:.2f} MB ({processed_df.shape[0]} rows x {processed_df.shape[1]} cols)\")\n",
        "\n",
        "# Save preprocessing pipeline objects\n",
        "preprocessors_dir = PROJECT_PATHS['preprocessors']\n",
        "os.makedirs(preprocessors_dir, exist_ok=True)\n",
        "\n",
        "# Save label encoders\n",
        "if 'label_encoders' in PREPROCESSING_PIPELINE:\n",
        "    with open(f\"{preprocessors_dir}/label_encoders.pkl\", 'wb') as f:\n",
        "        pickle.dump(PREPROCESSING_PIPELINE['label_encoders'], f)\n",
        "    print(f\"Saved label_encoders.pkl\")\n",
        "\n",
        "# Save scaler\n",
        "if 'scaler' in PREPROCESSING_PIPELINE:\n",
        "    with open(f\"{preprocessors_dir}/scaler.pkl\", 'wb') as f:\n",
        "        pickle.dump(PREPROCESSING_PIPELINE['scaler'], f)\n",
        "    print(f\"Saved scaler.pkl\")\n",
        "\n",
        "# Save preprocessing configuration\n",
        "preprocessing_config = {\n",
        "    'feature_columns': PREPROCESSING_PIPELINE['feature_columns'],\n",
        "    'preprocessing_steps': list(PREPROCESSING_PIPELINE.keys()),\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'random_state': CONFIG['random_state']\n",
        "}\n",
        "\n",
        "with open(f\"{preprocessors_dir}/preprocessing_config.json\", 'w') as f:\n",
        "    json.dump(preprocessing_config, f, indent=2)\n",
        "print(f\"Saved preprocessing_config.json\")\n",
        "\n",
        "print(f\"\\nAll preprocessed files saved to: {preprocessed_dir}\")\n",
        "\n",
        "# ==========================================\n",
        "# 7. QUALITY ASSESSMENT\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[QUALITY ASSESSMENT] Post-preprocessing quality evaluation...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def assess_preprocessing_quality():\n",
        "    \"\"\"Assess quality of preprocessing results\"\"\"\n",
        "    quality_report = {}\n",
        "\n",
        "    for split_name in ['train', 'val', 'test']:\n",
        "        X = PROCESSED_SPLITS[split_name]['X']\n",
        "        y = PROCESSED_SPLITS[split_name]['y']\n",
        "\n",
        "        missing_count = X.isnull().sum().sum()\n",
        "        numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        inf_count = np.isinf(X.select_dtypes(include=[np.number])).sum().sum()\n",
        "\n",
        "        split_quality = {\n",
        "            'shape': X.shape,\n",
        "            'missing_values': int(missing_count),\n",
        "            'infinite_values': int(inf_count),\n",
        "            'numeric_features': len(numeric_cols),\n",
        "            'target_encoded': not isinstance(y.iloc[0], str) if len(y) > 0 else True\n",
        "        }\n",
        "\n",
        "        quality_report[split_name] = split_quality\n",
        "\n",
        "        print(f\"{split_name.upper()} quality:\")\n",
        "        print(f\"  Shape: {X.shape}\")\n",
        "        print(f\"  Missing values: {missing_count}\")\n",
        "        print(f\"  Infinite values: {inf_count}\")\n",
        "\n",
        "    return quality_report\n",
        "\n",
        "QUALITY_REPORT = assess_preprocessing_quality()\n",
        "\n",
        "# ==========================================\n",
        "# 8. ENHANCED PUBLICATION-READY VISUALIZATIONS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[VISUALIZATIONS] Generating publication-ready analysis plots...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def generate_clinical_eda_visualization():\n",
        "    \"\"\"Generate comprehensive clinical EDA visualization\"\"\"\n",
        "    results_dir = create_results_directory(\"eda_analysis\")\n",
        "\n",
        "    # Set publication-ready style\n",
        "    plt.style.use('default')\n",
        "    plt.rcParams.update({\n",
        "        'figure.dpi': 300,\n",
        "        'font.size': 9,\n",
        "        'axes.titlesize': 11,\n",
        "        'axes.labelsize': 9,\n",
        "        'xtick.labelsize': 8,\n",
        "        'ytick.labelsize': 8,\n",
        "        'legend.fontsize': 8,\n",
        "        'figure.titlesize': 13\n",
        "    })\n",
        "\n",
        "    # Create comprehensive clinical analysis plot with proper spacing\n",
        "    fig = plt.figure(figsize=(18, 14))\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3,\n",
        "                         left=0.08, right=0.95, top=0.93, bottom=0.08)\n",
        "\n",
        "    # Section 1: Target distributions across splits\n",
        "    for idx, split_name in enumerate(['train', 'val', 'test']):\n",
        "        ax = fig.add_subplot(gs[0, idx])\n",
        "        y = PROCESSED_SPLITS[split_name]['y']\n",
        "        target_counts = pd.Series(y).value_counts().sort_index()\n",
        "\n",
        "        colors = ['#2E86AB', '#A23B72']\n",
        "        bars = ax.bar(['Non-ASD', 'ASD'], target_counts.values, color=colors, alpha=0.8)\n",
        "        ax.set_title(f'{split_name.title()} Set Distribution\\n(n={len(y):,})',\n",
        "                    fontweight='bold', pad=10)\n",
        "        ax.set_ylabel('Count')\n",
        "\n",
        "        # Add statistical annotations with proper positioning\n",
        "        for bar, count in zip(bars, target_counts.values):\n",
        "            percentage = (count / len(y)) * 100\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(target_counts)*0.02,\n",
        "                   f'{count:,}\\n({percentage:.1f}%)', ha='center', va='bottom',\n",
        "                   fontweight='bold', fontsize=8)\n",
        "\n",
        "        # Set proper y-axis limits to prevent overflow\n",
        "        ax.set_ylim(0, max(target_counts) * 1.15)\n",
        "\n",
        "    # Section 2: Behavioral features analysis\n",
        "    behavioral_data = CLINICAL_EDA['behavioral_features_analysis']\n",
        "    if 'individual_statistics' in behavioral_data and behavioral_data['individual_statistics']:\n",
        "        ax = fig.add_subplot(gs[1, :])\n",
        "\n",
        "        features = list(behavioral_data['individual_statistics'].keys())\n",
        "        cohens_d_values = [behavioral_data['individual_statistics'][f]['cohens_d'] for f in features]\n",
        "        p_values = [behavioral_data['individual_statistics'][f]['statistical_test'].get('p_value', 1.0) for f in features]\n",
        "\n",
        "        # Color bars by significance\n",
        "        colors = ['#FF6B6B' if p < 0.05 else '#87CEEB' for p in p_values]\n",
        "        bars = ax.bar(features, cohens_d_values, color=colors, alpha=0.8)\n",
        "\n",
        "        ax.set_title('Behavioral Features Effect Sizes (Cohen\\'s d)\\nRed: Significant (p<0.05), Blue: Non-significant',\n",
        "                    fontweight='bold', pad=15)\n",
        "        ax.set_xlabel('Behavioral Features (A1-A10)')\n",
        "        ax.set_ylabel('Cohen\\'s d')\n",
        "\n",
        "        # Add reference lines\n",
        "        ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Medium Effect')\n",
        "        ax.axhline(y=0.8, color='black', linestyle='--', alpha=0.7, label='Large Effect')\n",
        "        ax.legend(loc='upper right')\n",
        "\n",
        "        # Add significance annotations with proper positioning\n",
        "        max_val = max(cohens_d_values) if cohens_d_values else 1.0\n",
        "        for bar, p_val in zip(bars, p_values):\n",
        "            if p_val < 0.001:\n",
        "                sig_text = '***'\n",
        "            elif p_val < 0.01:\n",
        "                sig_text = '**'\n",
        "            elif p_val < 0.05:\n",
        "                sig_text = '*'\n",
        "            else:\n",
        "                sig_text = 'ns'\n",
        "\n",
        "            y_pos = bar.get_height() + max_val * 0.05\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, y_pos, sig_text,\n",
        "                   ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
        "\n",
        "        # Set proper limits to prevent overflow\n",
        "        ax.set_ylim(0, max_val * 1.2)\n",
        "\n",
        "    # Section 3: Clinical scales comparison\n",
        "    assessment_data = CLINICAL_EDA['clinical_assessment_analysis']\n",
        "    if assessment_data:\n",
        "        ax = fig.add_subplot(gs[2, 0])\n",
        "\n",
        "        scales = list(assessment_data.keys())\n",
        "        asd_means = [assessment_data[scale]['asd_mean'] for scale in scales]\n",
        "        non_asd_means = [assessment_data[scale]['non_asd_mean'] for scale in scales]\n",
        "\n",
        "        x = np.arange(len(scales))\n",
        "        width = 0.35\n",
        "\n",
        "        bars1 = ax.bar(x - width/2, non_asd_means, width, label='Non-ASD',\n",
        "                      color='#2E86AB', alpha=0.8)\n",
        "        bars2 = ax.bar(x + width/2, asd_means, width, label='ASD',\n",
        "                      color='#A23B72', alpha=0.8)\n",
        "\n",
        "        ax.set_title('Clinical Assessment Scales\\nMean Scores by ASD Status',\n",
        "                    fontweight='bold', pad=10)\n",
        "        ax.set_xlabel('Assessment Scales')\n",
        "        ax.set_ylabel('Mean Score')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(scales, rotation=0)\n",
        "        ax.legend()\n",
        "\n",
        "    # Section 4: Redundancy analysis summary\n",
        "    ax = fig.add_subplot(gs[2, 1])\n",
        "    redundancy_counts = [\n",
        "        len(REDUNDANCY_REPORT['correlation_analysis'].get('high_corr_pairs', [])),\n",
        "        len(REDUNDANCY_REPORT['variance_analysis'].get('low_variance_features', [])),\n",
        "        len(REDUNDANCY_REPORT['duplicate_analysis'].get('exact_duplicates', []))\n",
        "    ]\n",
        "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "    bars = ax.bar(['High Correlation\\n(r>0.95)', 'Low Variance\\n(<0.01)', 'Exact\\nDuplicates'],\n",
        "                 redundancy_counts, color=colors, alpha=0.8)\n",
        "    ax.set_title('Feature Redundancy Analysis', fontweight='bold', pad=10)\n",
        "    ax.set_ylabel('Count')\n",
        "\n",
        "    # Add count labels with proper positioning\n",
        "    max_count = max(redundancy_counts) if redundancy_counts else 1\n",
        "    for bar, count in zip(bars, redundancy_counts):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max_count * 0.02,\n",
        "               f'{count}', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
        "\n",
        "    ax.set_ylim(0, max_count * 1.15)\n",
        "\n",
        "    # Section 5: Data quality metrics\n",
        "    ax = fig.add_subplot(gs[2, 2])\n",
        "    quality_metrics = ['Missing Values', 'Infinite Values', 'Duplicates']\n",
        "    quality_counts = [\n",
        "        sum(QUALITY_REPORT[split]['missing_values'] for split in ['train', 'val', 'test']),\n",
        "        sum(QUALITY_REPORT[split]['infinite_values'] for split in ['train', 'val', 'test']),\n",
        "        CLINICAL_EDA['dataset_overview']['data_quality']['duplicate_rows']\n",
        "    ]\n",
        "\n",
        "    colors = ['#90EE90' if count == 0 else '#FFB6C1' for count in quality_counts]\n",
        "    bars = ax.bar(quality_metrics, quality_counts, color=colors, alpha=0.8)\n",
        "    ax.set_title('Data Quality Assessment\\nGreen: Clean, Pink: Issues',\n",
        "                fontweight='bold', pad=10)\n",
        "    ax.set_ylabel('Count')\n",
        "\n",
        "    # Add count labels with proper positioning\n",
        "    max_quality = max(quality_counts) if any(quality_counts) else 1\n",
        "    for bar, count in zip(bars, quality_counts):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max_quality * 0.02,\n",
        "               f'{count}', ha='center', va='bottom', fontweight='bold', fontsize=8)\n",
        "\n",
        "    ax.set_ylim(0, max_quality * 1.15)\n",
        "\n",
        "    # Add overall figure title with proper spacing\n",
        "    fig.suptitle('ASD Dataset: Publication-Ready Clinical Exploratory Data Analysis',\n",
        "                fontsize=14, fontweight='bold', y=0.97)\n",
        "\n",
        "    # Save EDA plot\n",
        "    eda_path = f\"{results_dir}/clinical_eda_analysis.png\"\n",
        "    plt.savefig(eda_path, dpi=300, bbox_inches='tight', facecolor='white',\n",
        "                edgecolor='none', pad_inches=0.3)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Clinical EDA analysis saved: {eda_path}\")\n",
        "    return results_dir\n",
        "\n",
        "def generate_correlation_matrix_visualization():\n",
        "    \"\"\"Generate professional triangular correlation matrix\"\"\"\n",
        "    results_dir = create_results_directory(\"eda_analysis\")\n",
        "\n",
        "    # Set publication-ready style for correlation matrix\n",
        "    plt.style.use('default')\n",
        "    plt.rcParams.update({\n",
        "        'figure.dpi': 300,\n",
        "        'font.size': 9,\n",
        "        'axes.titlesize': 12,\n",
        "        'axes.labelsize': 9,\n",
        "        'xtick.labelsize': 8,\n",
        "        'ytick.labelsize': 8,\n",
        "        'figure.titlesize': 14\n",
        "    })\n",
        "\n",
        "    if 'correlation_matrix' in REDUNDANCY_REPORT['correlation_analysis']:\n",
        "        corr_matrix = REDUNDANCY_REPORT['correlation_analysis']['correlation_matrix']\n",
        "        clinical_groups = REDUNDANCY_REPORT['clinical_analysis']['clinical_groups']\n",
        "\n",
        "        # Create ordered feature list by clinical groups\n",
        "        ordered_features = []\n",
        "        for group_name in ['behavioral_core', 'assessment_scales', 'demographic', 'medical_history']:\n",
        "            group_features = clinical_groups.get(group_name, [])\n",
        "            group_features_in_matrix = [f for f in group_features if f in corr_matrix.columns]\n",
        "            ordered_features.extend(group_features_in_matrix)\n",
        "\n",
        "        # Add remaining features not in any group\n",
        "        remaining_features = [f for f in corr_matrix.columns if f not in ordered_features]\n",
        "        ordered_features.extend(remaining_features)\n",
        "\n",
        "        if len(ordered_features) > 1 and not corr_matrix.empty:\n",
        "            # Ensure ordered features exist in correlation matrix\n",
        "            valid_ordered_features = [f for f in ordered_features if f in corr_matrix.columns]\n",
        "            if len(valid_ordered_features) > 1:\n",
        "                corr_matrix_ordered = corr_matrix.loc[valid_ordered_features, valid_ordered_features]\n",
        "\n",
        "                # Create figure with proper sizing for triangular matrix\n",
        "                fig, ax = plt.subplots(figsize=(14, 12))\n",
        "\n",
        "                # Create triangular mask (upper triangle)\n",
        "                mask = np.triu(np.ones_like(corr_matrix_ordered.values, dtype=bool), k=1)\n",
        "\n",
        "                # Apply mask to correlation matrix\n",
        "                corr_masked = corr_matrix_ordered.values.copy()\n",
        "                corr_masked[mask] = np.nan\n",
        "\n",
        "                # Create triangular correlation heatmap\n",
        "                im = ax.imshow(corr_masked, cmap='RdBu_r', aspect='equal',\n",
        "                              vmin=-1, vmax=1, interpolation='nearest')\n",
        "\n",
        "                # Set ticks and labels\n",
        "                n_features = len(corr_matrix_ordered.columns)\n",
        "                ax.set_xticks(range(n_features))\n",
        "                ax.set_yticks(range(n_features))\n",
        "                ax.set_xticklabels(corr_matrix_ordered.columns, rotation=45, ha='right')\n",
        "                ax.set_yticklabels(corr_matrix_ordered.index)\n",
        "\n",
        "                # Add correlation values only for lower triangle with significance threshold\n",
        "                for i in range(n_features):\n",
        "                    for j in range(n_features):\n",
        "                        if i > j:  # Lower triangle only\n",
        "                            corr_val = corr_matrix_ordered.iloc[i, j]\n",
        "                            if abs(corr_val) >= 0.3:  # Only show significant correlations\n",
        "                                # Choose text color based on correlation strength\n",
        "                                text_color = 'white' if abs(corr_val) > 0.7 else 'black'\n",
        "                                ax.text(j, i, f'{corr_val:.2f}', ha='center', va='center',\n",
        "                                       color=text_color, fontsize=8, fontweight='bold')\n",
        "\n",
        "                # Add clinical group separators\n",
        "                group_boundaries = []\n",
        "                current_pos = 0\n",
        "                group_labels = []\n",
        "\n",
        "                for group_name in ['behavioral_core', 'assessment_scales', 'demographic', 'medical_history']:\n",
        "                    group_features = [f for f in clinical_groups.get(group_name, [])\n",
        "                                    if f in valid_ordered_features]\n",
        "                    if group_features:\n",
        "                        group_labels.append((group_name.replace('_', ' ').title(),\n",
        "                                           current_pos, current_pos + len(group_features)))\n",
        "                        current_pos += len(group_features)\n",
        "                        if current_pos < len(valid_ordered_features):\n",
        "                            group_boundaries.append(current_pos - 0.5)\n",
        "\n",
        "                # Draw group separator lines\n",
        "                for boundary in group_boundaries:\n",
        "                    ax.axhline(y=boundary, color='white', linewidth=2.5)\n",
        "                    ax.axvline(x=boundary, color='white', linewidth=2.5)\n",
        "\n",
        "                # Add group labels on the right side\n",
        "                for group_name, start_pos, end_pos in group_labels:\n",
        "                    mid_pos = (start_pos + end_pos - 1) / 2\n",
        "                    ax.text(n_features + 0.5, mid_pos, group_name, rotation=90,\n",
        "                           ha='left', va='center', fontweight='bold', fontsize=10,\n",
        "                           color='darkblue')\n",
        "\n",
        "                # Add colorbar with proper positioning\n",
        "                cbar = plt.colorbar(im, ax=ax, shrink=0.8, pad=0.05, aspect=30)\n",
        "                cbar.set_label('Pearson Correlation Coefficient', rotation=270, labelpad=20)\n",
        "\n",
        "                # Improve layout and remove upper triangle visual noise\n",
        "                ax.set_xlim(-0.5, n_features - 0.5)\n",
        "                ax.set_ylim(n_features - 0.5, -0.5)\n",
        "\n",
        "                # Set title with proper spacing\n",
        "                ax.set_title('Feature Correlation Matrix with Clinical Grouping\\n' +\n",
        "                           '(Lower Triangle: |r| >= 0.3 displayed, Upper Triangle: Hidden)',\n",
        "                           fontweight='bold', pad=25)\n",
        "\n",
        "                # Adjust layout to prevent label cutoff\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # Save triangular correlation matrix\n",
        "                corr_path = f\"{results_dir}/triangular_correlation_matrix.png\"\n",
        "                plt.savefig(corr_path, dpi=300, bbox_inches='tight', facecolor='white',\n",
        "                           edgecolor='none', pad_inches=0.3)\n",
        "                plt.close()\n",
        "\n",
        "                print(f\"Triangular correlation matrix saved: {corr_path}\")\n",
        "                return results_dir\n",
        "\n",
        "    print(\"Warning: Correlation matrix data not available\")\n",
        "    return results_dir\n",
        "\n",
        "def generate_publication_visualizations():\n",
        "    \"\"\"Generate both EDA and correlation matrix as separate visualizations\"\"\"\n",
        "    # Generate clinical EDA visualization\n",
        "    eda_results_dir = generate_clinical_eda_visualization()\n",
        "\n",
        "    # Generate correlation matrix visualization\n",
        "    corr_results_dir = generate_correlation_matrix_visualization()\n",
        "\n",
        "    print(f\"Both visualizations saved in: {eda_results_dir}\")\n",
        "    return eda_results_dir\n",
        "\n",
        "# Generate enhanced visualizations\n",
        "VISUALIZATION_DIR = generate_publication_visualizations()\n",
        "\n",
        "# ==========================================\n",
        "# 9. EXPORT COMPREHENSIVE RESULTS\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[EXPORT] Saving comprehensive analysis results...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "def export_comprehensive_results():\n",
        "    \"\"\"Export all analysis results with clinical insights (JSON-safe version)\"\"\"\n",
        "\n",
        "    # Helper function to make objects JSON serializable\n",
        "    def make_json_serializable(obj):\n",
        "        \"\"\"Convert complex objects to JSON-serializable format\"\"\"\n",
        "        if isinstance(obj, pd.DataFrame):\n",
        "            return obj.to_dict()\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, (np.int64, np.int32)):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, (np.float64, np.float32)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, dict):\n",
        "            return {key: make_json_serializable(value) for key, value in obj.items()}\n",
        "        elif isinstance(obj, list):\n",
        "            return [make_json_serializable(item) for item in obj]\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    # Create JSON-safe copy of clinical EDA results\n",
        "    clinical_eda_safe = {}\n",
        "    try:\n",
        "        clinical_eda_safe = make_json_serializable(CLINICAL_EDA)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Clinical EDA results conversion failed: {e}\")\n",
        "        clinical_eda_safe = {'error': 'Conversion failed', 'message': str(e)}\n",
        "\n",
        "    # Create JSON-safe copy of redundancy report\n",
        "    redundancy_safe = {}\n",
        "    try:\n",
        "        redundancy_temp = REDUNDANCY_REPORT.copy()\n",
        "\n",
        "        # Handle correlation matrices separately\n",
        "        if 'correlation_analysis' in redundancy_temp:\n",
        "            if 'correlation_matrix' in redundancy_temp['correlation_analysis']:\n",
        "                corr_matrix = redundancy_temp['correlation_analysis']['correlation_matrix']\n",
        "                if isinstance(corr_matrix, pd.DataFrame):\n",
        "                    redundancy_temp['correlation_analysis']['correlation_matrix'] = corr_matrix.to_dict()\n",
        "\n",
        "        # Handle clinical analysis group correlations\n",
        "        if 'clinical_analysis' in redundancy_temp:\n",
        "            if 'group_correlations' in redundancy_temp['clinical_analysis']:\n",
        "                for group_name, group_data in redundancy_temp['clinical_analysis']['group_correlations'].items():\n",
        "                    if 'correlation_matrix' in group_data:\n",
        "                        corr_matrix = group_data['correlation_matrix']\n",
        "                        if isinstance(corr_matrix, pd.DataFrame):\n",
        "                            redundancy_temp['clinical_analysis']['group_correlations'][group_name]['correlation_matrix'] = corr_matrix.to_dict()\n",
        "\n",
        "        redundancy_safe = make_json_serializable(redundancy_temp)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Redundancy analysis conversion failed: {e}\")\n",
        "        redundancy_safe = {'error': 'Conversion failed', 'message': str(e)}\n",
        "\n",
        "    comprehensive_results = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'cell_info': {\n",
        "            'cell_number': 2,\n",
        "            'cell_name': 'Enhanced EDA & Preprocessing Pipeline (Publication-Ready)',\n",
        "            'methodology': 'Clinical-focused EDA with statistical rigor and IEEE visualization standards',\n",
        "            'enhancements': [\n",
        "                'Publication-ready clinical analysis',\n",
        "                'Statistical significance testing',\n",
        "                'Effect size calculations',\n",
        "                'Enhanced correlation matrix with clinical grouping',\n",
        "                'IEEE conference standard visualizations',\n",
        "                'Preprocessed CSV generation for reproducibility'\n",
        "            ]\n",
        "        },\n",
        "        'dataset_info': {\n",
        "            'splits_loaded': {split: list(SPLITS_DATA[split].shape) for split in SPLITS_DATA.keys()},\n",
        "            'ethnicity_removed': True,\n",
        "            'preprocessing_completed': True,\n",
        "            'clinical_focus': True,\n",
        "            'preprocessed_files_saved': {\n",
        "                'train': f\"{PROJECT_PATHS['preprocessed']}/train_set_preprocessed.csv\",\n",
        "                'val': f\"{PROJECT_PATHS['preprocessed']}/val_set_preprocessed.csv\",\n",
        "                'test': f\"{PROJECT_PATHS['preprocessed']}/test_set_preprocessed.csv\"\n",
        "            }\n",
        "        },\n",
        "        'clinical_eda_results': clinical_eda_safe,\n",
        "        'redundancy_analysis': redundancy_safe,\n",
        "        'quality_assessment': make_json_serializable(QUALITY_REPORT),\n",
        "        'preprocessing_pipeline': {\n",
        "            'components': list(PREPROCESSING_PIPELINE.keys()),\n",
        "            'feature_columns': PREPROCESSING_PIPELINE['feature_columns']\n",
        "        },\n",
        "        'processed_shapes': {\n",
        "            split: {\n",
        "                'features': list(PROCESSED_SPLITS[split]['X'].shape),\n",
        "                'target': len(PROCESSED_SPLITS[split]['y'])\n",
        "            } for split in PROCESSED_SPLITS.keys()\n",
        "        },\n",
        "        'publication_ready': {\n",
        "            'ieee_compliant_visualizations': True,\n",
        "            'statistical_rigor': True,\n",
        "            'clinical_interpretation': True,\n",
        "            'effect_sizes_calculated': True,\n",
        "            'preprocessed_csv_saved': True\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Save comprehensive results with error handling\n",
        "    results_path = f\"{VISUALIZATION_DIR}/comprehensive_eda_results.json\"\n",
        "    try:\n",
        "        save_results(comprehensive_results, results_path)\n",
        "        print(f\"Comprehensive results saved: {results_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving comprehensive results: {e}\")\n",
        "        # Save a simplified version\n",
        "        simplified_results = {\n",
        "            'timestamp': comprehensive_results['timestamp'],\n",
        "            'cell_info': comprehensive_results['cell_info'],\n",
        "            'dataset_info': comprehensive_results['dataset_info'],\n",
        "            'publication_ready': comprehensive_results['publication_ready'],\n",
        "            'error_note': 'Full results export failed, simplified version saved'\n",
        "        }\n",
        "        try:\n",
        "            save_results(simplified_results, results_path)\n",
        "            print(f\"Simplified results saved: {results_path}\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Failed to save even simplified results: {e2}\")\n",
        "\n",
        "    # Save preprocessing pipeline for reuse\n",
        "    pipeline_path = f\"{VISUALIZATION_DIR}/preprocessing_pipeline.json\"\n",
        "    try:\n",
        "        pipeline_export = {}\n",
        "        for key, value in PREPROCESSING_PIPELINE.items():\n",
        "            if key == 'feature_columns':\n",
        "                pipeline_export[key] = value\n",
        "            elif hasattr(value, 'get_params'):\n",
        "                pipeline_export[key] = {\n",
        "                    'type': type(value).__name__,\n",
        "                    'params': value.get_params()\n",
        "                }\n",
        "            else:\n",
        "                pipeline_export[key] = str(type(value))\n",
        "\n",
        "        save_results(pipeline_export, pipeline_path)\n",
        "        print(f\"Preprocessing pipeline info saved: {pipeline_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving preprocessing pipeline: {e}\")\n",
        "\n",
        "    return comprehensive_results\n",
        "\n",
        "COMPREHENSIVE_RESULTS = export_comprehensive_results()\n",
        "\n",
        "# ==========================================\n",
        "# 10. COMPLETION SUMMARY\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n[SUMMARY] Enhanced Cell 2 Completion Summary\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"DATA PROCESSING STATUS:\")\n",
        "print(f\"  Splits loaded: train ({SPLITS_DATA['train'].shape[0]}), val ({SPLITS_DATA['val'].shape[0]}), test ({SPLITS_DATA['test'].shape[0]})\")\n",
        "print(f\"  Features processed: {PROCESSED_SPLITS['train']['X'].shape[1]}\")\n",
        "print(f\"  Ethnicity bias: Removed\")\n",
        "print(f\"  Missing values: {sum(QUALITY_REPORT[split]['missing_values'] for split in QUALITY_REPORT.keys())}\")\n",
        "\n",
        "print(\"\\nPUBLICATION-READY CLINICAL ANALYSIS:\")\n",
        "behavioral_stats = CLINICAL_EDA['behavioral_features_analysis'].get('individual_statistics', {})\n",
        "significant_behavioral = sum(1 for f in behavioral_stats.values() if f['statistical_test'].get('significant', False))\n",
        "print(f\"  Behavioral features analyzed: {len(behavioral_stats)}\")\n",
        "print(f\"  Statistically significant features: {significant_behavioral}\")\n",
        "print(f\"  Clinical scales analyzed: {len(CLINICAL_EDA['clinical_assessment_analysis'])}\")\n",
        "print(f\"  Effect sizes calculated: Yes (Cohen's d)\")\n",
        "\n",
        "print(\"\\nREDUNDANCY ANALYSIS:\")\n",
        "print(f\"  High correlation pairs: {len(REDUNDANCY_REPORT['correlation_analysis'].get('high_corr_pairs', []))}\")\n",
        "print(f\"  Low variance features: {len(REDUNDANCY_REPORT['variance_analysis'].get('low_variance_features', []))}\")\n",
        "print(f\"  Clinical grouping completed: {len(REDUNDANCY_REPORT['clinical_analysis']['clinical_groups'])} groups\")\n",
        "\n",
        "print(\"\\nENHANCED PREPROCESSING PIPELINE:\")\n",
        "print(f\"  Method: Fit on train, transform on all splits\")\n",
        "print(f\"  Components: {len(PREPROCESSING_PIPELINE)} pipeline components\")\n",
        "print(f\"  Quality check: All splits processed without missing/infinite values\")\n",
        "print(f\"  Clinical feature preservation: Yes\")\n",
        "\n",
        "print(\"\\nPREPROCESSED CSV FILES SAVED:\")\n",
        "print(f\"  Directory: {PROJECT_PATHS['preprocessed']}\")\n",
        "print(f\"  Files generated:\")\n",
        "print(f\"    - train_set_preprocessed.csv\")\n",
        "print(f\"    - val_set_preprocessed.csv\")\n",
        "print(f\"    - test_set_preprocessed.csv\")\n",
        "print(f\"  Preprocessing objects saved to: {PROJECT_PATHS['preprocessors']}\")\n",
        "\n",
        "print(\"\\nPUBLICATION OUTPUTS:\")\n",
        "print(f\"  Results directory: {VISUALIZATION_DIR}\")\n",
        "print(f\"  IEEE-compliant visualizations: Yes (300 DPI)\")\n",
        "print(f\"  Statistical annotations: Yes\")\n",
        "print(f\"  Clinical interpretations: Yes\")\n",
        "print(f\"  JSON exports: 2 (comprehensive results + pipeline)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENHANCED CELL 2 COMPLETED SUCCESSFULLY\")\n",
        "print(\"Publication-ready processed data, clinical insights, and preprocessed CSV files generated\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g8vzvl6EtVx",
        "outputId": "c3151c3c-3c26-45b7-c9d5-a48fa91d28ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ASD DETECTION PROJECT: BASELINE XGBOOST IMPLEMENTATION\n",
            "Cell 2: Enhanced EDA & Preprocessing Pipeline (Publication-Ready)\n",
            "================================================================================\n",
            "\n",
            "[VERIFICATION] Checking Cell 1 completion...\n",
            "--------------------------------------------------\n",
            "Cell 1 variables loaded successfully\n",
            "\n",
            "[DATA LOADING] Loading cleaned split datasets...\n",
            "--------------------------------------------------\n",
            "Loaded train set: 1,270 samples x 26 features\n",
            "Loaded val set: 318 samples x 26 features\n",
            "Loaded test set: 397 samples x 26 features\n",
            "\n",
            "Feature-target separation completed\n",
            "Feature columns: 25\n",
            "\n",
            "[PUBLICATION EDA] Comprehensive clinical analysis (train set only)...\n",
            "--------------------------------------------------\n",
            "Section 1: Dataset Overview & Quality Assessment\n",
            "Section 2: Clinical Target Analysis\n",
            "Section 3: Behavioral Features Analysis (A1-A10)\n",
            "Section 4: Clinical Assessment Scales\n",
            "Section 5: Demographic Analysis\n",
            "\n",
            "[REDUNDANCY ANALYSIS] Comprehensive feature redundancy assessment...\n",
            "--------------------------------------------------\n",
            "Step 1: Correlation analysis\n",
            "  Converting binary categorical Speech Delay/Language Disorder for analysis\n",
            "  Converting binary categorical Learning disorder for analysis\n",
            "  Converting binary categorical Genetic_Disorders for analysis\n",
            "  Converting binary categorical Depression for analysis\n",
            "  Converting binary categorical Global developmental delay/intellectual disability for analysis\n",
            "  Converting binary categorical Social/Behavioural Issues for analysis\n",
            "  Converting binary categorical Anxiety_disorder for analysis\n",
            "  Converting binary categorical Jaundice for analysis\n",
            "  Converting binary categorical Family_mem_with_ASD for analysis\n",
            "  Analyzing 23 numeric/binary features for correlation\n",
            "  Found 20 high correlation pairs (>0.95)\n",
            "Step 2: Low variance analysis\n",
            "  Found 0 low variance features (<0.01)\n",
            "Step 3: Duplicate feature detection\n",
            "  Found 0 exact duplicate feature pairs\n",
            "Step 4: Clinical feature grouping\n",
            "  behavioral_core: 10 features (10 numeric), avg correlation: 0.472\n",
            "  assessment_scales: 3 features (3 numeric), avg correlation: 0.385\n",
            "  demographic: 3 features (2 numeric), avg correlation: 0.123\n",
            "  medical_history: 7 features (7 numeric), avg correlation: 0.842\n",
            "\n",
            "[PREPROCESSING] Implementing fit-transform pipeline...\n",
            "--------------------------------------------------\n",
            "Step 1: Feature type identification\n",
            "  Numeric columns: 14\n",
            "  Categorical columns: 11\n",
            "Step 2: Missing value imputation\n",
            "  Numeric imputer fitted (median strategy)\n",
            "  Categorical imputer fitted (mode strategy)\n",
            "Step 3: Categorical encoding\n",
            "  Label encoders fitted for 11 categorical features\n",
            "Step 4: Feature scaling\n",
            "  Scaler fitted for 4 continuous features\n",
            "Step 5: Target encoding\n",
            "Applying preprocessing to train set...\n",
            "  train preprocessing completed: (1270, 25)\n",
            "Applying preprocessing to val set...\n",
            "  val preprocessing completed: (318, 25)\n",
            "Applying preprocessing to test set...\n",
            "  test preprocessing completed: (397, 25)\n",
            "Preprocessing pipeline completed for all splits\n",
            "\n",
            "[SAVE PREPROCESSED] Saving preprocessed datasets to CSV...\n",
            "--------------------------------------------------\n",
            "Saved train_set_preprocessed.csv: 0.17 MB (1270 rows x 26 cols)\n",
            "Saved val_set_preprocessed.csv: 0.04 MB (318 rows x 26 cols)\n",
            "Saved test_set_preprocessed.csv: 0.05 MB (397 rows x 26 cols)\n",
            "Saved label_encoders.pkl\n",
            "Saved scaler.pkl\n",
            "Saved preprocessing_config.json\n",
            "\n",
            "All preprocessed files saved to: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/01_Dataset/splits/no_ethnicity/preprocessed\n",
            "\n",
            "[QUALITY ASSESSMENT] Post-preprocessing quality evaluation...\n",
            "--------------------------------------------------\n",
            "TRAIN quality:\n",
            "  Shape: (1270, 25)\n",
            "  Missing values: 0\n",
            "  Infinite values: 0\n",
            "VAL quality:\n",
            "  Shape: (318, 25)\n",
            "  Missing values: 0\n",
            "  Infinite values: 0\n",
            "TEST quality:\n",
            "  Shape: (397, 25)\n",
            "  Missing values: 0\n",
            "  Infinite values: 0\n",
            "\n",
            "[VISUALIZATIONS] Generating publication-ready analysis plots...\n",
            "--------------------------------------------------\n",
            "Clinical EDA analysis saved: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/03_Results/baseline_experiments/eda_analysis/clinical_eda_analysis.png\n",
            "Triangular correlation matrix saved: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/03_Results/baseline_experiments/eda_analysis/triangular_correlation_matrix.png\n",
            "Both visualizations saved in: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/03_Results/baseline_experiments/eda_analysis\n",
            "\n",
            "[EXPORT] Saving comprehensive analysis results...\n",
            "--------------------------------------------------\n",
            "Comprehensive results saved: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/03_Results/baseline_experiments/eda_analysis/comprehensive_eda_results.json\n",
            "Preprocessing pipeline info saved: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/03_Results/baseline_experiments/eda_analysis/preprocessing_pipeline.json\n",
            "\n",
            "[SUMMARY] Enhanced Cell 2 Completion Summary\n",
            "================================================================================\n",
            "DATA PROCESSING STATUS:\n",
            "  Splits loaded: train (1270), val (318), test (397)\n",
            "  Features processed: 25\n",
            "  Ethnicity bias: Removed\n",
            "  Missing values: 0\n",
            "\n",
            "PUBLICATION-READY CLINICAL ANALYSIS:\n",
            "  Behavioral features analyzed: 9\n",
            "  Statistically significant features: 9\n",
            "  Clinical scales analyzed: 3\n",
            "  Effect sizes calculated: Yes (Cohen's d)\n",
            "\n",
            "REDUNDANCY ANALYSIS:\n",
            "  High correlation pairs: 20\n",
            "  Low variance features: 0\n",
            "  Clinical grouping completed: 4 groups\n",
            "\n",
            "ENHANCED PREPROCESSING PIPELINE:\n",
            "  Method: Fit on train, transform on all splits\n",
            "  Components: 6 pipeline components\n",
            "  Quality check: All splits processed without missing/infinite values\n",
            "  Clinical feature preservation: Yes\n",
            "\n",
            "PREPROCESSED CSV FILES SAVED:\n",
            "  Directory: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/01_Dataset/splits/no_ethnicity/preprocessed\n",
            "  Files generated:\n",
            "    - train_set_preprocessed.csv\n",
            "    - val_set_preprocessed.csv\n",
            "    - test_set_preprocessed.csv\n",
            "  Preprocessing objects saved to: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/01_Dataset/preprocessors\n",
            "\n",
            "PUBLICATION OUTPUTS:\n",
            "  Results directory: /content/drive/MyDrive/ASD_GWO_XGBoost_Project/03_Results/baseline_experiments/eda_analysis\n",
            "  IEEE-compliant visualizations: Yes (300 DPI)\n",
            "  Statistical annotations: Yes\n",
            "  Clinical interpretations: Yes\n",
            "  JSON exports: 2 (comprehensive results + pipeline)\n",
            "\n",
            "================================================================================\n",
            "ENHANCED CELL 2 COMPLETED SUCCESSFULLY\n",
            "Publication-ready processed data, clinical insights, and preprocessed CSV files generated\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}